# NLP and Deep Learning All-in-One Part III: Transformer, BERT, and XLNet
Attention Based Models Interview Questions
1. Why choose Attention based models over Recurrent based ones?
2. What is Attention? What’s wrong with seq2seq model?
3. What’s Self-Attention?
4. How to implement Self-Attention?
5. What is Transformer? What is Multi-head self-attention?
6. What is BERT? Why choose Bert over Embedding models?
7. What’s the difference between BERT and other traditional language models? or Why is Masked Language Modeling more effective than Sequential Language Modeling?
8. What’s the flaw of Transformer? How does BERT solve that problem?
9. How does BERT do classification?
10. What type of classification tasks can BERT do?
11. How to do BERT fine-tuning? what hyper-parameters does BERT use?
12. What’s wrong with BERT? What’s its limitations?
13. What is XLNet? Why Choose XLNet over BERT?
14. What’s the difference between BERT and XLNet [CLS] and [SEP] pattern?