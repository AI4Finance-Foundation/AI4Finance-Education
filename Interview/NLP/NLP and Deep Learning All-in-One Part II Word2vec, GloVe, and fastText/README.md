# NLP and Deep Learning All-in-One Part II: Word2vec, GloVe, and fastText
Embedding Based Models Interview Questions
1. What are some of the traditional ways to represent words in numeric vectors?
2. What is Word Embedding or Word2vec?
3. What are the 2 architectures of Word2vec?
4. How to train Word2vec (Skip-gram)?
5. What are the pros and cons for Word2vec?
6. What is GloVe? How is GloVe different from Word2vec?
7. What is fastText? How is fastText different from Word2vec?
8. Why choose fastText over Word2vec?
9. How to handle Out-of-Vocabulary words?


<div align="center">
<img width="1337" alt="WechatIMG3528" src="https://user-images.githubusercontent.com/31713746/198068277-91547504-4fd3-411c-ad9e-e0b96ea96c82.png">
</div>
